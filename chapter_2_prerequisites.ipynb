{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93761f78",
   "metadata": {},
   "source": [
    "## 第二章的主要内容围绕一些简单的pytorch使用和基础数学知识展开，是后续学习的基础"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f4c4ff",
   "metadata": {},
   "source": [
    "### 2.1 数据操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11ae8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "torch.Size([12])\n",
      "12\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "tensor([[ 0.8435, -0.4141, -1.5065],\n",
      "        [ 0.3616, -0.7020, -1.5590]])\n",
      "tensor([[1, 2, 3],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 一些最为基础的pytorch数值计算函数示意\n",
    "import torch\n",
    "\n",
    "x = torch.arange(12)\n",
    "print(x)\n",
    "# torch.arange创建一个行向量，行向量在未指定开始整数时默认为0，共计12个元素，即0-11\n",
    "\n",
    "print(x.shape)\n",
    "# 通过访问pytorch中基础运算单元tensor的属性shape可以得到当前tensor的数值形状\n",
    "\n",
    "print(x.numel())\n",
    "# numel()方法可以得到当前tensor的所有内部元素个数\n",
    "\n",
    "x = x.reshape((3,4))\n",
    "print(x)\n",
    "# reshape方法重新创建一个符合定义形状的tensor，因此如果希望更改当前tensor，需要重新赋值一次\n",
    "\n",
    "print(torch.zeros(2,3,4)) #torch.zeros((2,3,4))\n",
    "# zeros或ones定义一个全是0或者全是1的tensor，形状符合定义\n",
    "\n",
    "print(torch.randn((2,3))) \n",
    "# randn定义一个符合规定形状的遵从标准正态分布的tensor数组\n",
    "\n",
    "a = [[1,2,3],[3,4,5]]\n",
    "print(torch.tensor(a))\n",
    "# torch.tensor将一个ndarray、普通列表等不是tensor的运算单元转换为torch里面的tensor数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563143c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.,  4.,  6., 10.]) tensor([ 1.,  0., -2., -6.]) tensor([ 2.,  4.,  8., 16.]) tensor([2.0000, 1.0000, 0.5000, 0.2500]) tensor([  2.,   4.,  16., 256.])\n",
      "tensor([7.3891, 7.3891, 7.3891, 7.3891])\n",
      "tensor([0.6931, 0.6931, 0.6931, 0.6931])\n",
      "tensor(8.)\n",
      "tensor(8.)\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]]) tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "# 既然torch.tensor创建的都是一些计算单元，tensor可以参与各种各样的运算，因此这部分介绍了一些基本的数学运算\n",
    "\n",
    "x = torch.ones(4) * 2\n",
    "y = torch.tensor([1.0, 2, 4, 8])\n",
    "print(x + y, x - y, x * y, x / y, x ** y)\n",
    "# 上述加减乘除求幂运算均基于相同元素位置操作的计算原则\n",
    "\n",
    "print(torch.exp(x))\n",
    "print(torch.log(x))\n",
    "# 可以按照元素位置操作的原则定义一些其他的计算\n",
    "\n",
    "print(x.sum())\n",
    "print(torch.abs(x).sum())\n",
    "# 按元素计算的方式可以遍历整个tensor，通过sum()方法计算tensor的所有元素之和，那当然也可以用这样的方式计算L1范数\n",
    "\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))  \n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(torch.cat((X,Y),dim = 0),torch.cat((X,Y),dim = 1))\n",
    "# torch.cat的这个方法和之前的一些定义tensor的方法不同，在定义tensor的方法中，指代形状的多维维数可以组合成一个list或tuple，但也可以分开，但在torch.cat中一定需要组合在一起，可以\"转到定义\"查看具体的使用方法\n",
    "\n",
    "X == Y\n",
    "# 有数值计算，tensor自然也支持逻辑运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59bcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 脱胎于numpy.ndarray，pytorch中的tensor计算单元同样有特殊的广播机制，这个机制能帮助我们在代码中简化很多操作，但有时不注意也会造成一些不必要的问题\n",
    "\n",
    "a = torch.arange(3).reshape((3, 1))  \n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "print(a)\n",
    "print(a.shape)\n",
    "print(b)\n",
    "print(b.shape)\n",
    "# 两个tensor的形状不相同，但我们仍然可以让这两个tensor参与前面一样的按元素运算\n",
    "\n",
    "print(a + b)\n",
    "# 两个tensor在形状不同的情况下，使用按元素计算会自动的在不足的那个维度上复制自动填充，达到相同形状按元素运算的效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c36657f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.,  9., 10., 11.])\n",
      "tensor([[ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  9.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "# 类似其他python数组，tensor也可以有索引切片等基本操作\n",
    "\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4)) \n",
    "print(X[-1])\n",
    "print(X[1:3])\n",
    "# 这里的切片只指定了一个维度，那么就只关心第一个维度的指定切片，其他的所有维度全额保留\n",
    "\n",
    "\n",
    "X[1, 2] = 9\n",
    "print(X)\n",
    "# 通过索引改写tensor当中的单个元素\n",
    "\n",
    "X[0:2, :] = 12\n",
    "print(X)\n",
    "# 上述的改写方式等同于X[0:2] = 12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16378615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 这一部分是一个实用的知识点介绍，根据tensor对象的创建原理，可以通过分析tensor的存储地址来得到更为高效的tensor创建改写方式，目的是为了节省内存\n",
    "\n",
    "before = id(Y)\n",
    "Y = Y + X\n",
    "print(before == id(Y))\n",
    "# 输出False,原因是 Y = Y + X 会重新创建一个Y，这个Y不再是原来的Y，相当于并未在去掉原来Y的基础上重新存储了一个变量，这在内存中就造成了不必要的存储，下面推荐两种不同的原地操作方式，都能保证在原来的tensor上进行操作\n",
    "\n",
    "Z = torch.arange(12).reshape(3,4)\n",
    "before_z = id(Z)\n",
    "Z[:] = Z + Y\n",
    "print(id(Z) == before_z)\n",
    "# 这里相当于通过索引的方式只是改写了每个元素的值\n",
    "\n",
    "M = torch.arange(12,dtype = torch.float).reshape(3,4)\n",
    "before_m = id(M)\n",
    "M += Y\n",
    "print(id(M) == before_m)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741d92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "tensor(6) <class 'torch.Tensor'>\n",
      "6 6.0 6\n"
     ]
    }
   ],
   "source": [
    "# pytorch中的tensor对象作为专属的计算单元，也可以与python中的其他基础数据结构进行转换，最为常规的就是和numpy.ndarray的相互转换和普通的float int list等\n",
    "\n",
    "a = torch.tensor([1,2,3])\n",
    "print(type(a))\n",
    "\n",
    "b = a.numpy()\n",
    "print(type(b))\n",
    "\n",
    "c = a.sum()\n",
    "print(c, type(c))\n",
    "print(c.item(), float(c), int(c))\n",
    "# 将单个元素的tensor变成普通的标量通常使用item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aeba6b",
   "metadata": {},
   "source": [
    "### 2.2 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a8e42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "1       2.0   NaN  106000\n",
      "2       4.0   NaN  178100\n"
     ]
    }
   ],
   "source": [
    "# 现实场景中数据多数时候都不是由使用者去定义的，数据通常位于一个额外的文件中，同时原始数据大概率并未进行标准化处理，因此我们首先应该读取这些成规模的数据，同时学会对原始数据做初步乃至更加细致的预处理，数据清洗和预处理是训练网络很重要的一个环节\n",
    "\n",
    "# 本节会借助一个新的python包 pandas pandas帮助定义数据、对数据进行初步处理\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_dirs = os.path.join(\".\",\"data\")\n",
    "os.makedirs(data_dirs, exist_ok=True)\n",
    "# os包使用到的是os.path.join()和os.makedirs() 前者的使用方式跟字符串的join方法类似，后者的作用是创建一个目录，参数exist_ok指定如果当前的目录已存在，也默认创建\n",
    "data_file = os.path.join(data_dirs,\"tiny_data.csv\")\n",
    "with open(data_file,\"w\") as f:\n",
    "    f.write(\"NumRooms,Alley,Price\\n\")\n",
    "    f.write(\"NA,Pave,127500\\n\")\n",
    "    f.write(\"2,NA,106000\\n\")\n",
    "    f.write(\"4,NA,178100\\n\")\n",
    "# 这里存储数据的格式为CSV文件，此文件格式通过逗号分隔不同数据类型，通过‘\\n’换行 这里的写入方式是一类常规写法\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f2d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms\n",
      "0       NaN\n",
      "1       2.0\n",
      "2       4.0\n",
      "   NumRooms\n",
      "0       3.0\n",
      "1       2.0\n",
      "2       4.0\n",
      "  Alley\n",
      "0  Pave\n",
      "1   NaN\n",
      "2   NaN\n",
      "   Alley_Pave\n",
      "0        True\n",
      "1       False\n",
      "2       False\n",
      "   Alley_Pave  Alley_nan\n",
      "0        True      False\n",
      "1       False       True\n",
      "2       False       True\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]]) tensor([[127500.],\n",
      "        [106000.],\n",
      "        [178100.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个数据文件之后，在使用之前需对其进行预处理，这里只介绍一些比较简单的预处理，包括填补空白内容和对字符内容设置类似bool型的存储方式(因为参与深度学习中字符串会通过一定的方式编码转换为数字形式参与计算)\n",
    "\n",
    "inputs_1, inputs_2, outputs = data.iloc[:,0:1], data.iloc[:,1:2], data.iloc[:,2:3]\n",
    "print(inputs_1)\n",
    "# 以pandas包打开的文件，索引的方式就是iloc，这里的\n",
    "input_1 = inputs_1.fillna(inputs_1.mean())\n",
    "print(input_1)\n",
    "# 通过fill_na填充NAN, 填充的值可以用mean()平均值替代\n",
    "\n",
    "print(inputs_2)\n",
    "inputs_2 = pd.get_dummies(inputs_2,dummy_na=False)\n",
    "print(inputs_2)\n",
    "inputs_2 = data.iloc[:,1:2]\n",
    "inputs_2 = pd.get_dummies(inputs_2,dummy_na=True)\n",
    "print(inputs_2)\n",
    "# 通过get_dummy将这类非0即1的字符串转换为bool类型，参数dummy_na是True或是False有较大的区别，如果是True会将原来的字符串变为两列，是NAN的赋值为1，如果是False则没有复制操作，是NAN的赋值为0\n",
    "\n",
    "x_2 = torch.tensor(inputs_2.to_numpy(),dtype=torch.float32)\n",
    "y = torch.tensor(outputs.to_numpy(dtype=float))\n",
    "print(x_2,y)\n",
    "# 先将pandas的数据格式转换为numpy中的ndarray，再由numpy.ndarray转换为torch.tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f183559e",
   "metadata": {},
   "source": [
    "### 2.3 线性代数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38351419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在深度学习中常用的数学基础知识就是线性代数、概率论和统计学、优化等\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.tensor(3.0)\n",
    "b = torch.tensor(2.0)\n",
    "\n",
    "print(a + b, a * b, a / b)\n",
    "# 上述tensor定义的是单个元素，其数学意义即普通标量\n",
    "\n",
    "x = torch.arange(4)\n",
    "print(x,x[3])\n",
    "# 上述tensor定义的是单个向量，向量中的单个分量为标量\n",
    "print(len(x),x.shape)\n",
    "# 向量相较于标量就有一些不同的函数和方法可以调用\n",
    "\n",
    "A = torch.arange(20).reshape(4,5)\n",
    "# 将向量拓展到二维即矩阵，矩阵相较于向量的操作多了不少，基于矩阵本身的性质可以计算矩阵秩、奇异值、行列式、转置等，最简单不借助其他方法和函数的就有转置操作\n",
    "print(A)\n",
    "print(A.T)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l_pytorch_pure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
