{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93761f78",
   "metadata": {},
   "source": [
    "## 第二章的主要内容围绕一些简单的pytorch使用和基础数学知识展开，是后续学习的基础"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f4c4ff",
   "metadata": {},
   "source": [
    "### 2.1 数据操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11ae8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "torch.Size([12])\n",
      "12\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "tensor([[ 0.8435, -0.4141, -1.5065],\n",
      "        [ 0.3616, -0.7020, -1.5590]])\n",
      "tensor([[1, 2, 3],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 一些最为基础的pytorch数值计算函数示意\n",
    "import torch\n",
    "\n",
    "x = torch.arange(12)\n",
    "print(x)\n",
    "# torch.arange创建一个行向量，行向量在未指定开始整数时默认为0，共计12个元素，即0-11\n",
    "\n",
    "print(x.shape)\n",
    "# 通过访问pytorch中基础运算单元tensor的属性shape可以得到当前tensor的数值形状\n",
    "\n",
    "print(x.numel())\n",
    "# numel()方法可以得到当前tensor的所有内部元素个数\n",
    "\n",
    "x = x.reshape((3,4))\n",
    "print(x)\n",
    "# reshape方法重新创建一个符合定义形状的tensor，因此如果希望更改当前tensor，需要重新赋值一次\n",
    "\n",
    "print(torch.zeros(2,3,4)) #torch.zeros((2,3,4))\n",
    "# zeros或ones定义一个全是0或者全是1的tensor，形状符合定义\n",
    "\n",
    "print(torch.randn((2,3))) \n",
    "# randn定义一个符合规定形状的遵从标准正态分布的tensor数组\n",
    "\n",
    "a = [[1,2,3],[3,4,5]]\n",
    "print(torch.tensor(a))\n",
    "# torch.tensor将一个ndarray、普通列表等不是tensor的运算单元转换为torch里面的tensor数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563143c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.,  4.,  6., 10.]) tensor([ 1.,  0., -2., -6.]) tensor([ 2.,  4.,  8., 16.]) tensor([2.0000, 1.0000, 0.5000, 0.2500]) tensor([  2.,   4.,  16., 256.])\n",
      "tensor([7.3891, 7.3891, 7.3891, 7.3891])\n",
      "tensor([0.6931, 0.6931, 0.6931, 0.6931])\n",
      "tensor(8.)\n",
      "tensor(8.)\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]]) tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "# 既然torch.tensor创建的都是一些计算单元，tensor可以参与各种各样的运算，因此这部分介绍了一些基本的数学运算\n",
    "\n",
    "x = torch.ones(4) * 2\n",
    "y = torch.tensor([1.0, 2, 4, 8])\n",
    "print(x + y, x - y, x * y, x / y, x ** y)\n",
    "# 上述加减乘除求幂运算均基于相同元素位置操作的计算原则\n",
    "\n",
    "print(torch.exp(x))\n",
    "print(torch.log(x))\n",
    "# 可以按照元素位置操作的原则定义一些其他的计算\n",
    "\n",
    "print(x.sum())\n",
    "print(torch.abs(x).sum())\n",
    "# 按元素计算的方式可以遍历整个tensor，通过sum()方法计算tensor的所有元素之和，那当然也可以用这样的方式计算L1范数\n",
    "\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))  \n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(torch.cat((X,Y),dim = 0),torch.cat((X,Y),dim = 1))\n",
    "# torch.cat的这个方法和之前的一些定义tensor的方法不同，在定义tensor的方法中，指代形状的多维维数可以组合成一个list或tuple，但也可以分开，但在torch.cat中一定需要组合在一起，可以\"转到定义\"查看具体的使用方法\n",
    "\n",
    "X == Y\n",
    "# 有数值计算，tensor自然也支持逻辑运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59bcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 脱胎于numpy.ndarray，pytorch中的tensor计算单元同样有特殊的广播机制，这个机制能帮助我们在代码中简化很多操作，但有时不注意也会造成一些不必要的问题\n",
    "\n",
    "a = torch.arange(3).reshape((3, 1))  \n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "print(a)\n",
    "print(a.shape)\n",
    "print(b)\n",
    "print(b.shape)\n",
    "# 两个tensor的形状不相同，但我们仍然可以让这两个tensor参与前面一样的按元素运算\n",
    "\n",
    "print(a + b)\n",
    "# 两个tensor在形状不同的情况下，使用按元素计算会自动的在不足的那个维度上复制自动填充，达到相同形状按元素运算的效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c36657f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.,  9., 10., 11.])\n",
      "tensor([[ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  9.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "# 类似其他python数组，tensor也可以有索引切片等基本操作\n",
    "\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4)) \n",
    "print(X[-1])\n",
    "print(X[1:3])\n",
    "# 这里的切片只指定了一个维度，那么就只关心第一个维度的指定切片，其他的所有维度全额保留\n",
    "\n",
    "\n",
    "X[1, 2] = 9\n",
    "print(X)\n",
    "# 通过索引改写tensor当中的单个元素\n",
    "\n",
    "X[0:2, :] = 12\n",
    "print(X)\n",
    "# 上述的改写方式等同于X[0:2] = 12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16378615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 这一部分是一个实用的知识点介绍，根据tensor对象的创建原理，可以通过分析tensor的存储地址来得到更为高效的tensor创建改写方式，目的是为了节省内存\n",
    "\n",
    "before = id(Y)\n",
    "Y = Y + X\n",
    "print(before == id(Y))\n",
    "# 输出False,原因是 Y = Y + X 会重新创建一个Y，这个Y不再是原来的Y，相当于并未在去掉原来Y的基础上重新存储了一个变量，这在内存中就造成了不必要的存储，下面推荐两种不同的原地操作方式，都能保证在原来的tensor上进行操作\n",
    "\n",
    "Z = torch.arange(12).reshape(3,4)\n",
    "before_z = id(Z)\n",
    "Z[:] = Z + Y\n",
    "print(id(Z) == before_z)\n",
    "# 这里相当于通过索引的方式只是改写了每个元素的值\n",
    "\n",
    "M = torch.arange(12,dtype = torch.float).reshape(3,4)\n",
    "before_m = id(M)\n",
    "M += Y\n",
    "print(id(M) == before_m)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741d92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "tensor(6) <class 'torch.Tensor'>\n",
      "6 6.0 6\n"
     ]
    }
   ],
   "source": [
    "# pytorch中的tensor对象作为专属的计算单元，也可以与python中的其他基础数据结构进行转换，最为常规的就是和numpy.ndarray的相互转换和普通的float int list等\n",
    "\n",
    "a = torch.tensor([1,2,3])\n",
    "print(type(a))\n",
    "\n",
    "b = a.numpy()\n",
    "print(type(b))\n",
    "\n",
    "c = a.sum()\n",
    "print(c, type(c))\n",
    "print(c.item(), float(c), int(c))\n",
    "# 将单个元素的tensor变成普通的标量通常使用item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aeba6b",
   "metadata": {},
   "source": [
    "### 2.2 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a8e42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "1       2.0   NaN  106000\n",
      "2       4.0   NaN  178100\n"
     ]
    }
   ],
   "source": [
    "# 现实场景中数据多数时候都不是由使用者去定义的，数据通常位于一个额外的文件中，同时原始数据大概率并未进行标准化处理，因此我们首先应该读取这些成规模的数据，同时学会对原始数据做初步乃至更加细致的预处理，数据清洗和预处理是训练网络很重要的一个环节\n",
    "\n",
    "# 本节会借助一个新的python包 pandas pandas帮助定义数据、对数据进行初步处理\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_dirs = os.path.join(\".\",\"data\")\n",
    "os.makedirs(data_dirs, exist_ok=True)\n",
    "# os包使用到的是os.path.join()和os.makedirs() 前者的使用方式跟字符串的join方法类似，后者的作用是创建一个目录，参数exist_ok指定如果当前的目录已存在，也默认创建\n",
    "data_file = os.path.join(data_dirs,\"tiny_data.csv\")\n",
    "with open(data_file,\"w\") as f:\n",
    "    f.write(\"NumRooms,Alley,Price\\n\")\n",
    "    f.write(\"NA,Pave,127500\\n\")\n",
    "    f.write(\"2,NA,106000\\n\")\n",
    "    f.write(\"4,NA,178100\\n\")\n",
    "# 这里存储数据的格式为CSV文件，此文件格式通过逗号分隔不同数据类型，通过‘\\n’换行 这里的写入方式是一类常规写法\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f2d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms\n",
      "0       NaN\n",
      "1       2.0\n",
      "2       4.0\n",
      "   NumRooms\n",
      "0       3.0\n",
      "1       2.0\n",
      "2       4.0\n",
      "  Alley\n",
      "0  Pave\n",
      "1   NaN\n",
      "2   NaN\n",
      "   Alley_Pave\n",
      "0        True\n",
      "1       False\n",
      "2       False\n",
      "   Alley_Pave  Alley_nan\n",
      "0        True      False\n",
      "1       False       True\n",
      "2       False       True\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]]) tensor([[127500.],\n",
      "        [106000.],\n",
      "        [178100.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个数据文件之后，在使用之前需对其进行预处理，这里只介绍一些比较简单的预处理，包括填补空白内容和对字符内容设置类似bool型的存储方式(因为参与深度学习中字符串会通过一定的方式编码转换为数字形式参与计算)\n",
    "\n",
    "inputs_1, inputs_2, outputs = data.iloc[:,0:1], data.iloc[:,1:2], data.iloc[:,2:3]\n",
    "print(inputs_1)\n",
    "# 以pandas包打开的文件，索引的方式就是iloc，这里的\n",
    "input_1 = inputs_1.fillna(inputs_1.mean())\n",
    "print(input_1)\n",
    "# 通过fill_na填充NAN, 填充的值可以用mean()平均值替代\n",
    "\n",
    "print(inputs_2)\n",
    "inputs_2 = pd.get_dummies(inputs_2,dummy_na=False)\n",
    "print(inputs_2)\n",
    "inputs_2 = data.iloc[:,1:2]\n",
    "inputs_2 = pd.get_dummies(inputs_2,dummy_na=True)\n",
    "print(inputs_2)\n",
    "# 通过get_dummy将这类非0即1的字符串转换为bool类型，参数dummy_na是True或是False有较大的区别，如果是True会将原来的字符串变为两列，是NAN的赋值为1，如果是False则没有复制操作，是NAN的赋值为0\n",
    "\n",
    "x_2 = torch.tensor(inputs_2.to_numpy(),dtype=torch.float32)\n",
    "y = torch.tensor(outputs.to_numpy(dtype=float))\n",
    "print(x_2,y)\n",
    "# 先将pandas的数据格式转换为numpy中的ndarray，再由numpy.ndarray转换为torch.tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f183559e",
   "metadata": {},
   "source": [
    "### 2.3 线性代数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38351419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.) tensor(6.) tensor(1.5000)\n",
      "tensor([0, 1, 2, 3]) tensor(3)\n",
      "4 torch.Size([4])\n",
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19]])\n",
      "tensor([[ 0,  5, 10, 15],\n",
      "        [ 1,  6, 11, 16],\n",
      "        [ 2,  7, 12, 17],\n",
      "        [ 3,  8, 13, 18],\n",
      "        [ 4,  9, 14, 19]])\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n"
     ]
    }
   ],
   "source": [
    "# 在深度学习中常用的数学基础知识就是线性代数、概率论和统计学、优化等\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.tensor(3.0)\n",
    "b = torch.tensor(2.0)\n",
    "\n",
    "print(a + b, a * b, a / b)\n",
    "# 上述tensor定义的是单个元素，其数学意义即普通标量\n",
    "\n",
    "x = torch.arange(4)\n",
    "print(x,x[3])\n",
    "# 上述tensor定义的是单个向量，向量中的单个分量为标量\n",
    "print(len(x),x.shape)\n",
    "# 向量相较于标量就有一些不同的函数和方法可以调用\n",
    "\n",
    "A = torch.arange(20).reshape(4,5)\n",
    "# 将向量拓展到二维即矩阵，矩阵相较于向量的操作多了不少，基于矩阵本身的性质可以计算矩阵秩、奇异值、行列式、转置等，最简单不借助其他方法和函数的就有转置操作\n",
    "print(A)\n",
    "print(A.T)\n",
    "\n",
    "X = torch.arange(24).reshape(2,3,4)\n",
    "print(X)\n",
    "# 如果将向量拓展到更高维度，将其称为张量，高阶向量在矩阵和向量的基础上可以参与更多基础计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a36fc5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor([3., 3., 3.])\n",
      "None\n",
      "tensor([7., 7., 7.])\n",
      "tensor([1., 2., 3.], grad_fn=<CopyBackwards>)\n",
      "tensor([[  0.,   1.,   4.,   9.],\n",
      "        [ 16.,  25.,  36.,  49.],\n",
      "        [ 64.,  81., 100., 121.],\n",
      "        [144., 169., 196., 225.],\n",
      "        [256., 289., 324., 361.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor([[ 0.,  2.,  4.,  6.],\n",
      "        [ 8., 10., 12., 14.],\n",
      "        [16., 18., 20., 22.],\n",
      "        [24., 26., 28., 30.],\n",
      "        [32., 34., 36., 38.]]) tensor([[ 2.,  3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13.],\n",
      "        [14., 15., 16., 17.],\n",
      "        [18., 19., 20., 21.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/s82h6bc93fx7147qvyyp3dtw0000gn/T/ipykernel_87778/3062874849.py:20: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1753839011583/work/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(b.grad)\n"
     ]
    }
   ],
   "source": [
    "# 以张量作为基础运算单元，需要了解一些张量算法的基础性质，这里将d2l中几个小章节的张量运算都放在一起\n",
    "\n",
    "A = torch.arange(20,dtype=torch.float32).reshape(5,4)\n",
    "B = A.clone()\n",
    "print(A, B)\n",
    "\n",
    "# 使用clone()分配新内存，将A的一个副本分配给B\n",
    "# 关于这里的clone()可以补充一部分内容，b = a.clone()这里的写法要注意的是此时b不是叶子节点，因此b输出梯度是None,因为不是叶子节点并不记录梯度，clone得到的这个tensor拥有和原tensor一样的shape、device、dtype\n",
    "# 但不同的地方在于因为是原tensor的一个副本新分配了一个地址，两个tensor之间没有直接联系，两个tensor并不共享内存地址，不会两个值随时保持一致，同时，clone得到的tensor会将梯度叠加到原tensor上，效果如下面所示：\n",
    "\n",
    "a = torch.tensor([1,2,3],dtype=torch.float32, requires_grad=True)\n",
    "b = a.clone()\n",
    "\n",
    "c = a * 3\n",
    "d = b * 4\n",
    "\n",
    "c.sum().backward()\n",
    "print(a.grad)\n",
    "d.sum().backward()\n",
    "print(b.grad)\n",
    "print(a.grad)\n",
    "\n",
    "b = torch.zeros_like(a).copy_(a)\n",
    "print(b)\n",
    "# 这里使用到了copy_()来定义tensor,通过copy()得到的tensor拥有和clone()一致的特性，但不同之处就是两个方法的调用对象不同，因此在使用copy_()时需要提前创建tensor,这点也可以从方法名称看出\n",
    "\n",
    "c = a.detach()\n",
    "d = a.data\n",
    "# 严格来说，这里介绍的detach()方法和访问tensor.data的机制不相同，但因为和上面的clone()和copy_()运行机制不同，也可以放在一起介绍，在后面的内容就会出现.data访问tensor的取值的情况，因此可以直接取出.data创建一个值相同的tensor\n",
    "# 上述两个属性和方法创建相同的tensor的关键特性在于：这两者创建的tensor本身不具备梯度，他是一个脱离计算图的tensor创建方式，脱离的一个关键是你可以想象成是原tensor和复制得到的tensor之间的一个关系，detach()和.data相当于在两者之间砍了一刀，因此梯度不会发生回传，创建得到的新tensor本身不带有梯度，但如果此tensor需要后续赋予梯度，他也只会自己作为叶子节点保存自己的梯度，但新创建的tensor和原tensor共享内存，因此会时时刻刻相等\n",
    "\n",
    "# d2l在基础性质的小篇章中还额外介绍了标量和张量计算的规则和最基础的张量乘积，即Hadamard积，代码中如果没有额外定义，两个形状相同的tensor之间发生的乘法运算就是Hadamard积运算,即对应元素相乘得到结果\n",
    "print(A * B)\n",
    "x = 2\n",
    "print(A, x * A, x + A) # 标量和张量发生计算默认在每个元素上发生相同的运算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc3f4029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.]) tensor(6.)\n",
      "torch.Size([4]) torch.Size([])\n",
      "torch.Size([5, 4])\n",
      "torch.Size([4]) torch.Size([5])\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  6.,  8., 10.],\n",
      "        [12., 15., 18., 21.],\n",
      "        [24., 28., 32., 36.],\n",
      "        [40., 45., 50., 55.]])\n",
      "torch.Size([4])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# pytorch中有多种方式可以实现tensor降维，例如求和、按指定维度求和求均值等等，另外按指定维度求和如果完全遵循降维的标准也会发生一些问题，因此在求和时也可以选择不降维\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "print(x,x.sum())\n",
    "print(x.shape,x.sum().shape) # 发生降维\n",
    "\n",
    "print(A.shape) # A.shape为[5,4],可以通过这个tensor展示按照指定维度求和的结果\n",
    "print(A.sum(axis=0).shape, A.sum(axis=1).shape) # 第一个求和的形状为[4],第二个求和的形状为[5],从求和角度上分析，第一个求和实际上就是在第一个维度发生的计算，因此第一个维度被消除，第二个类似，下面说明求和的一个写法\n",
    "print(A.sum() == A.sum(axis=[0,1])) # 结果类似，因为此时指定的维度包括所有维度\n",
    "\n",
    "# mean()的用法和sum()相同，此处不再赘述\n",
    "print(A.mean() == A.sum() / A.numel())\n",
    "\n",
    "# sum()有一类计算是累积求和，即在原来的基础上不断累加，pytorch中这类计算的函数是cumsum(),cumsum()的使用方式和上面的sum()和mean()类似\n",
    "print(A) # 从0-19，按照[5,4]的形状排列\n",
    "print(A.cumsum(axis=0))\n",
    "\n",
    "# sum()之类的计算即然可以自动降维，自然可以手动地选择不降维，例如\n",
    "print(A.sum(axis=0).shape) # 求和结果的shape为[4]\n",
    "print(A.sum(axis=0,keepdim=True).shape) # 求和结果的shape为[1,4],即保留了求和的那个维度，只将求和的维度定义为1,这样的保留维度行为方便通过广播机制计算比例等，这在概率论和后续的自定义softmax中有展示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12540e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.]) torch.Size([4])\n",
      "tensor(6.) torch.Size([]) tensor(True)\n",
      "torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵中很基础的一个运算就是矩阵相乘，矩阵相乘需要遵循一定的原则，在pytorch中则区分的更细\n",
    "\n",
    "# 第一部分是向量之间的点积运算\n",
    "print(x, x.shape) # [0,1,2,3]的形状为[4]\n",
    "y = torch.ones(4, dtype=torch.float32)\n",
    "print(torch.dot(x,y),torch.dot(x,y).shape,torch.dot(x,y) == (x * y).sum()) # 通过dot()函数计算两个向量的点乘结果\n",
    "\n",
    "# 第二部分是矩阵和向量之间的运算，通过mv()函数实现\n",
    "print(A.shape)\n",
    "print(torch.mv(A,x).shape) # mv()实现矩阵和向量的乘法运算，和线性代数中的定义相同，矩阵的第二维需和向量的长度相同\n",
    "\n",
    "# 第三部分是矩阵乘法，通过mm()函数实现\n",
    "B = torch.ones(4,3)\n",
    "print(torch.mm(A,B).shape) # mm()实现两个矩阵的乘法，(5,4)*(4,3)=(5,3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "705206a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.) tensor(5.)\n",
      "tensor(7.) tensor(7.)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "# 后续的很多内容都会涉及到tensor的范数计算，范数不是单一定义的，但范数都满足线性代数中对于向量范数的定义，范数遵循三个基础性质：第一个是线性缩放，即任意范数都随着向量的常数倍缩放而缩放；第二个是三角不等式，即f(x+y)<=f(x)+f(y),第三个是范数非负\n",
    "\n",
    "# 通常接触的最多的是三类范数：1-范数、2-范数，2-范数在向量中通常定义为欧几里得距离、Frobenius范数\n",
    "\n",
    "# 2-范数在torch中实现的函数是torch.norm()\n",
    "u = torch.tensor([3.0, 4.0])\n",
    "print(torch.norm(u), u.norm()) # 两种写法效果相同\n",
    "\n",
    "# 1-范数不需要有专门的函数来定义，因为1-范数的数学形式完全可以通过一些基础的函数组合实现\n",
    "print(u.abs().sum(), torch.abs(u).sum())\n",
    "\n",
    "# Frobenius范数同样通过torch.norm()实现，通常在一个矩阵或张量中计算\n",
    "m = torch.ones(9,4)\n",
    "print(torch.norm(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f80bda",
   "metadata": {},
   "source": [
    "### 2.4 微积分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018fe8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (h = 0.10000, numerical limit = 2.30000\n",
      " (h = 0.01000, numerical limit = 2.03000\n",
      " (h = 0.00100, numerical limit = 2.00300\n",
      " (h = 0.00010, numerical limit = 2.00030\n",
      " (h = 0.00001, numerical limit = 2.00003\n"
     ]
    }
   ],
   "source": [
    "# 在深度学习中的很多环节都会用到微积分的部分知识，例如反向传播梯度的计算等等，因此可以利用一些python包帮助理解微积分中的一些概念，导数和微分就是微积分中最基础的概念\n",
    "\n",
    "import numpy as np \n",
    "from matplotlib_inline import backend_inline\n",
    "\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x\n",
    "\n",
    "def numerical_lim(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "# 定义一个简单的函数，另外，通过导数的定义设计numerical_lim函数，通过迭代缩小变化值计算更为精确的导数值\n",
    "\n",
    "h = 0.1\n",
    "for i in range(5):\n",
    "    print(\" (h = %.5f, numerical limit = %.5f\" % (h,numerical_lim(f,1,h)))\n",
    "    h *= 0.1\n",
    "\n",
    "# 这里在d2l原书中说明了复合函数的求导方式，都是一些高中就学过的知识，不再赘述，在这里介绍一下后续也会重复使用d2l包中的画图函数，因为复用性强且考虑到了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l_pytorch_pure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
